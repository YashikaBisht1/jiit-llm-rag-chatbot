# llm-tuning-with-rag
Retrieval-Augmented Generation chatbot for JIIT using fine-tuned Mistral-7B (Unsloth), LlamaIndex, and ROUGE evaluation.

# ğŸ” Fine-Tuning LLMs with RAG for University Q\&A â€” Inspired by IEEE Research (2024)

This project demonstrates how to fine-tune a lightweight language model (TinyLlama) on custom university FAQ data using Unsloth with LoRA/QLoRA, and enhance its retrieval capabilities using RAG (Retrieval-Augmented Generation). The PDF brochure of JIIT (jiit.ac.in) was parsed and embedded using FAISS and llama-index. The system is evaluated using ROUGE metrics for answer quality, and the fine-tuned model can be exported in GGUF format for CPU or Ollama use.

> ğŸ”¬ Based on:
> **"Fine-Tuning LLM to Answer Basic Questions for Prospective New Students at Syiah Kuala University Using the RAG Method"**
> [IEEE ICIC 2024 Â· DOI: 10.1109/ICIC64337.2024.10956296](https://doi.org/10.1109/ICIC64337.2024.10956296)


## ğŸ¯ Motivation

Rather than building LLMs from scratch, modern AI development focuses on **fine-tuning existing models on custom datasets** using efficient techniques like **LoRA**, **QLoRA**, and **DPO**. This project was inspired by the shift in thinking â€” from full-stack AI to **lean, strategic fine-tuning** â€” and explores:

* Fine-tuning with instruction-style data
* Retrieval-based answering using unstructured documents
* Evaluation with ROUGE

## ğŸ§  Project Overview

### ğŸ”„ Workflow

```text
Instruction-Tuning Dataset (JIIT FAQs)
         â”‚
         â–¼
LoRA / QLoRA Fine-Tuning (TinyLlama)
         â”‚
         â”œâ”€â”€ Optional: DPO with preference pairs
         â”‚
         â–¼
Document Ingestion (PDFs using llama-index)
         â”‚
         â–¼
RAG System (FAISS + Sentence Embeddings)
         â”‚
         â–¼
Evaluation (ROUGE Metrics)
  

## ğŸ“¦ Tech Stack

| Category    | Tool / Library                                 |
| ----------- | ---------------------------------------------- |
| LLM         | `TinyLlama-1.1B-Chat`                          |
| Fine-tuning | `Unsloth`, `PEFT`, `Transformers`, `TRL`       |
| RAG         | `llama-index`, `FAISS`, `SentenceTransformers` |
| Evaluation  | `rouge_score`                                  |
| Deployment  | `GGUF`, `Ollama-compatible`                    |
| Platform    | Google Colab (T4 GPU)                          |

---

## ğŸ“ Files and Structure

| File / Folder                            | Description                               |
| ---------------------------------------- | ----------------------------------------- |
| `jiit_finetune.json`                     | Instruction-tuning dataset (JIIT Q\&A)    |
| `qa_data.json`                           | Extracted question/reference answer pairs |
| `rag_data/`                              | Folder containing JIIT PDF brochure       |
| `Implementation_IEEE_research2024.ipynb` | Full end-to-end Colab notebook            |
| `tinyllama_dpo_model/`                   | Fine-tuned model directory (optional)     |
| `README.md`                              | Project overview and documentation        |


## ğŸ“Š Evaluation

The model is evaluated using **ROUGE-1, ROUGE-2, and ROUGE-L** metrics, comparing its generated answers with ground truth responses from the dataset.


## ğŸ§ª Example Query

```python
Q: What is the hostel fee per semester?
A: Hostel + mess charges are around â‚¹1â€¯lakh per semester for Indian students.
```

## ğŸ§  Key Learnings

* **Instruction-tuning** with well-structured prompts leads to high factual alignment.
* **Unsloth** allows 4-bit quantized fine-tuning on free GPUs like T4 with very low VRAM.
* **RAG** makes models dynamically answer queries beyond their training scope.
* **GGUF** format allows inference on CPUs with tools like `llama.cpp` and `Ollama`.


## ğŸ“š References

* IEEE Research: H. Rachmat et al., *â€œFine-Tuning LLM using RAGâ€*, ICIC 2024
* [LLM Engineerâ€™s Handbook](https://www.amazon.com/LLM-Engineers-Handbook-engineering-production/dp/1836200072)
* [Unsloth GitHub](https://github.com/unslothai/unsloth)

## ğŸ¤ Let's Connect

Are you working on:

* LLM fine-tuning
* Retrieval-Augmented Generation (RAG)
* EdTech AI assistants
* Efficient LLM deployment (GGUF, Ollama)

Feel free to open issues, share feedback, or connect on [LinkedIn](https://www.linkedin.com/in/).

